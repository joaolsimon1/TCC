---
title: "TCC"
output: html_document
date: "2024-02-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(zoo)
library(dtwclust)
library(caret)
```

```{=html}
<style>
body{
text-alig: justify;
}
</style>
```


- Importando o banco de dados do Wesley Cota

```{r}
# Importando os dados diarios
dados_temporais20 <- read.csv("cases-brazil-cities-time_2020.csv")
dados_temporais21 <- read.csv("cases-brazil-cities-time_2021.csv")
dados_temporais22 <- read.csv("cases-brazil-cities-time_2022.csv")

# Juntando os 3 anos em um unico dataset
dados_temporais <- rbind(dados_temporais20, dados_temporais21, dados_temporais22)

# Removendo do environment os anos separados para liberar memoria
rm(dados_temporais20)
rm(dados_temporais21)
rm(dados_temporais22)

# Removendo as linhas de TOTAIS  e os Casos sem localização definida
dados_temporais <- dados_temporais %>% 
                      filter(!grepl("TOTAL", city), !grepl("CASO SEM L", city))

# Vamos considerar apenas municipios com +100mil habitantes (grande porte)

## Para isso vamos ter que importar os dados pontuais do IBGE do censo de 2010
dados_pontuais <- readxl::read_xlsx("Data_municipality_variables.xlsx")
dados_pontuais <- dados_pontuais[-1,]

## Vetor contendo apenas o código dos municipios com +100 habitantes
muni_grande_porte <- dados_pontuais %>% filter(Population > 100000) %>% select(Codmun6)
dados_temporais$ibgeID <- substr(dados_temporais$ibgeID, 1, 6) # removendo o ultimo digito do codigo do IBGE

# Separando outro dataset apenas com os municipios que em 2010 tinham pop. > 100k habitantes
dados_temporais2 <- dados_temporais %>% filter(ibgeID %in% muni_grande_porte$Codmun6)
```


```{r}
# Separando a série temporal de novos casos de SP
serie_temporal <- dados_temporais2[dados_temporais2$city == "São Paulo/SP", "newCases"]

# Normalizando os dados da serie temporal para deixar as cidades padronizadas
media <- mean(serie_temporal)
desvio_padrao <- sd(serie_temporal)
serie_padronizada <- (serie_temporal - media) / desvio_padrao

plot.ts(dados_temporais2[dados_temporais2$city == "São Paulo/SP", "newCases"])
plot.ts(serie_padronizada)
```

# Por Dia

```{r}

# Calcular a média móvel de 7 dias
media_movel <- rollmean(serie_padronizada, k = 14, fill = NA)

# Criar um data frame para ggplot
df <- data.frame(Dias = seq_along(serie_padronizada),
                 Valor = serie_padronizada,
                 Media_Movel = media_movel)

# Criar o gráfico usando ggplot
ggplot(df, aes(x = Dias)) +
  geom_line(aes(y = Valor), color = "#378FC8") +  # Série temporal original
  geom_line(aes(y = Media_Movel), color = "#010401", size = 1) +  # Média móvel de 7 dias
  labs(x = "Dias", y = "Valor", title = "São Paulo/SP - Média Móvel de 7 dias de Novos Casos") +
  ylim(c(-1,3)) +
  theme_minimal()
```



```{r}
# Fazendo um dataframe apenas com as séries padronizadas padronizadas
dados_diarios_pd <- dados_temporais2

for (i in unique(dados_temporais2$city)) {
  serie_temporal <- dados_temporais2[dados_temporais2$city == i, "newCases"]

  # Normalizando os dados da serie temporal para deixar as cidades padronizadas
  media <- mean(serie_temporal)
  desvio_padrao <- sd(serie_temporal)
  serie_padronizada <- (serie_temporal - media) / desvio_padrao

  dados_diarios_pd[dados_diarios_pd$city == i, "newCases"] <- serie_padronizada
}


## Média movel de 14 dias padronizada
dados_diarios_SMA <- NULL
# Fazendo um dataframe apenas com as médias móveis das séries padronizadas
for (i in unique(dados_temporais2$city)) {
  serie <- dados_diarios_pd[dados_diarios_pd$city == i,]
  media_movel <- rollmean(serie$newCases, k = 14, fill = NA)
  
  dados_diarios_SMA <- rbind(dados_diarios_SMA, 
                                  dados_diarios_pd[dados_diarios_pd$city == i,])
  
  dados_diarios_SMA[dados_diarios_SMA$city == i, "newCases"] <- media_movel
}

plot.ts(dados_diarios_pd[dados_diarios_pd$city == "São Paulo/SP", "newCases"])
plot.ts(dados_diarios_SMA[dados_diarios_SMA$city == "São Paulo/SP", "newCases"])
```


```{r}
# Dados reorganizados
dados_diarios_reorganizados <- dados_diarios_SMA %>%
  select(date, newCases, city) %>% 
  pivot_wider(names_from = date, values_from = newCases, values_fill = 0)


dados_diarios_reorganizados[is.na(dados_diarios_reorganizados)] <- 0

df_final2 <- as.matrix(dados_diarios_reorganizados[,c(2:1042)])
rownames(df_final2) <- dados_diarios_reorganizados$city
```


## DTW

```{r}
cluster_assignments <- tsclust(df_final2, k = 2L:10L, seed = 8L, distance = "dtw_basic", centroid = "dba", norm = "L2", window.size = 30L)


validacao_n_clusters <- sapply(cluster_assignments, cvi, type = "internal")

cluster_assignments$cluster


#names(validacao_n_clusters) <- paste0("k_",2L:10L)
validacao_n_clusters

result <- tsclust(df_final2, k = 19, seed = 8L, distance = "dtw", centroid = 'dba', norm = "L2", window.size = 30L)
plot(result)  # Plot density of clusters



stats::hclust(df_final2, dist.method = 'dtw', method = 'ward.D2')


tscl
sqrt(326)

```






# Por SEMANA EPIDEMIOLÓGICA

```{r}
dados_semanais <- dados_temporais2 %>% 
                          group_by(epi_week, city) %>% 
                          summarise("newCases" = sum(newCases))


# Separando a série temporal de novos casos de SP
serie_semanal <- dados_semanais[dados_semanais$city == "São Leopoldo/RS", "newCases"]

# Normalizando os dados da serie temporal para deixar as cidades padronizadas
media <- mean(serie_semanal$newCases)
desvio_padrao <- sd(serie_semanal$newCases)
serie_semanal_padronizada <- (serie_semanal$newCases - media) / desvio_padrao



plot.ts(serie_semanal_padronizada)

#dados_temporais2[dados_temporais2$city == "São Félix do Xingu/PA",]
```



```{r}
dados_semanais_pd <- dados_semanais
for (i in unique(dados_semanais$city)) {
  serie_temporal <- dados_semanais[dados_semanais$city == i, "newCases"]

  # Normalizando os dados da serie temporal para deixar as cidades padronizadas
  media <- mean(serie_temporal$newCases)
  desvio_padrao <- sd(serie_temporal$newCases)
  serie_temporal$newCases <- (serie_temporal$newCases - media) / desvio_padrao
  
  # Substituir os dados padronizados de volta no data frame original
  #dados_semanais_pd[dados_semanais_pd$city == i, "newCases"] <- serie_temporal
  dados_semanais_pd$newCases[dados_semanais_pd$city == i] <- serie_temporal$newCases

  #dados_semanais_pd <- dados_semanais %>% mutate(ifelse(city == i, serie_temporal$newCases, newCases))
}


# Dados reorganizados
dados_semanais_reorganizados <- dados_semanais_pd %>%
  pivot_wider(names_from = epi_week, values_from = newCases, values_fill = 0)


df_final3 <- as.matrix(dados_semanais_reorganizados[,c(2:150)])
rownames(df_final3) <- dados_semanais_reorganizados$city

#agrupamento3 <- uhclust(df_final3)
```



## DTW

```{r}
cluster_assignments <- tsclust(df_final3, k = 2L:10L, seed = 8L, distance = "dtw_basic", centroid = "dba", norm = "L2", window.size = 5L)


validacao_n_clusters <- sapply(cluster_assignments, cvi, type = "internal")
validacao_n_clusters
cluster_assignments$cluster


names(cluster_assignments) <- paste0("k_",2L:10L)

result <- tsclust(df_final3, k = 3L, seed = 8L, distance = "dtw", centroid = "dba", norm = "L2")
plot(result)  # Plot density of clusters
```




```{r}

# Dataframe com os municipios e os resultado dos clusters
clusters <- data.frame("city" = rownames(df_final3), "Cluster" = result@cluster)

clusters <- inner_join(clusters,
                      dados_temporais2 %>% 
                        select(city, ibgeID) %>% 
                        group_by(city, ibgeID) %>% 
                        summarise(),
                      by = "city")

clusters <- inner_join(clusters,
                       municipios,
                       by = c("ibgeID"="Codmun6"))


```




# Métodos de Classificação

```{r, warning = FALSE, message = FALSE}
clusters

var_num <- names(clusters)[9:13]
var_quali <- names(clusters)[c(2,8)]

clusters[var_quali] <- lapply(clusters[var_quali], function(x) {as.factor(x)})
clusters[var_num] <- lapply(clusters[var_num], function(x) {as.numeric(x)})

clusters2 <- clusters[,c(2, 8:13)]

summary(clusters2)

str(dados_teste)
```


```{r, warning = FALSE, message = FALSE}
set.seed(324507)

## Random Forest
sample <- sample(seq_len(nrow(clusters2)), size = floor(0.8 * nrow(clusters2)))
dados_treino <- clusters2[sample, ]
dados_teste  <- clusters2[-sample, ]


controle <- trainControl(method = "cv", number = 5)

modelo1 <- train(Cluster ~ ., data = dados_treino, method = "rf", trControl = controle, metric = "Accuracy", set.seed = 0304)

predicoes1 <- predict(modelo1, newdata = dados_teste)
matriz_confusao <- confusionMatrix(predicoes1, dados_teste$Cluster)
print(matriz_confusao)
print(paste("Acurácia = ", " ", round(matriz_confusao[["overall"]][["Accuracy"]],4)," | " , "Erro de precisão ="," ", round((1 - matriz_confusao[["overall"]][["Accuracy"]]),4), sep = ""))
```




```{r}
## SVM
modelo2 <- train(Cluster ~ ., data = dados_treino, method = "svmLinear", trControl = controle, metric = "Accuracy", set.seed = 304)

predicoes2 <- predict(modelo2, newdata = dados_teste)
matriz_confusao <- confusionMatrix(predicoes2, dados_teste$Cluster)
print(matriz_confusao)
print(paste("Acurácia = ", " ", round(matriz_confusao[["overall"]][["Accuracy"]],4)," | " , "Erro de precisão ="," ", round((1 - matriz_confusao[["overall"]][["Accuracy"]]),4), sep = ""))
```



```{r}
## Boosting
modelo3 <- train(Cluster ~ ., data = dados_treino, method = "gbm", trControl = controle, metric = "Accuracy")

predicoes3 <- predict(modelo3, newdata = dados_teste)
matriz_confusao <- confusionMatrix(predicoes3, dados_teste$Cluster)
print(matriz_confusao)
print(paste("Acurácia = ", " ", round(matriz_confusao[["overall"]][["Accuracy"]],4)," | " , "Erro de precisão ="," ", round((1 - matriz_confusao[["overall"]][["Accuracy"]]),4), sep = ""))
```



```{r}

metodo <- data.frame()
acuracia <- data.frame()
proporcao <- data.frame()
grupo <- data.frame()
kappa <- data.frame()
for (i in c(0.5, 0.7, 0.8, 0.9)) {
  library(caret)
  library(glmnet)

  set.seed(304)
  ## Random Forest
  sample <- sample(seq_len(nrow(clusters2)), size = floor(i * nrow(clusters2)))
  dados_treino <- clusters2[sample, ]
  dados_teste  <- clusters2[-sample, ]
  controle <- trainControl(method = "cv", number = 5)
  
  for (j in c("Boosting", "svmLinear", "RandomForest")) {
    if (j == "svmLinear") {
      modelo <- train(Cluster ~ ., data = dados_treino, method = "svmLinear", trControl = controle, family = "binomial")
    }
    else if(j == "Boosting"){
      modelo <- train(Cluster ~ ., data = dados_treino, method = "gbm", trControl = controle, verbose = FALSE)
    }
    else if(j == "RandomForest"){
      modelo <- train(Cluster ~ ., data = dados_treino, method = "rf", trControl = controle)
    }
    predicoes <- predict(modelo, newdata = dados_teste)
    matriz_confusao <- confusionMatrix(predicoes, dados_teste$Cluster)
    # print("==========================================================")
    # print(paste("Método = ",j," | Proporção do banco de treinamento = ",i,sep = ""))
    # print(matriz_confusao)
    # print(paste("Acurácia = ", " ", round(matriz_confusao[["overall"]][["Accuracy"]],4)," | " , "Erro de precisão ="," ", round((1 - matriz_confusao[["overall"]][["Accuracy"]]),4), sep = ""))
    
    acuracia <- rbind(acuracia, round(matriz_confusao[["overall"]][["Accuracy"]],3))
    metodo <- rbind(metodo, j)
    proporcao <- rbind(proporcao, i)
    kappa <- rbind(kappa, round(matriz_confusao[["overall"]][["Kappa"]],3))
  }

  grupo <- cbind(acuracia, kappa, metodo, proporcao)
}
names(grupo) <- c("Acurácia", "Kappa", "Método", "Proporção Treino")

knitr::kable(grupo)

```

