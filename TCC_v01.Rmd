---
title: "TCC"
output:
  html_document:
    code_download: true
    theme: flatly
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(zoo)
library(dtwclust)
library(caret)
```

```{=html}
<style>
body{
text-alig: justify;
}
</style>
```

# Resumo

As análises feitas abaixo seguem tendo como base a metodoliga aplicada em [Time series clustering of COVID-19 pandemic-related data](https://www.sciencedirect.com/science/article/pii/S2666764923000115). Nela, o pesquisador seguiu os seguintes passos:

  - Obteve as séries temporais de casos diários em todos os países;

  - Normalizou todas as séries;

  - Ao invés de utilizar a série bruta normalizada, utilizou a média móvel centrada de 7 dias de cada país;

  - Selecionou o algoritmo DTW Barycenter para formar os clusters;

  - Utilizou o método não hierárquico para definir a quantidade de clusters;

  - Avaliou os cluster atráves de alguns Índices de Validação de Clusters (CVI); (Coeficiente da Silhueta e Índice de Dunn)

  - Como resultado final, pegou a média móvel de cada cluster e fez uma análise do comportamento da série e de alguns países que faziam parte do grupo formado;

<br>

Neste trabalho, foi seguida a seguinte medologia:

  - Foram obtidas as séries **semanais** de casos diários em todos os **municípios de grande porte** (população > 100 mil habitantes) que constavam na fonte de dados mais recente até o momento da pandemia (Censo de 2010), totalizando 326 municípios de um total de 5570 ($n=326; N=5570$).

  - Normalizamos todas as séries temporais; 
  
  $$z_{it}=\frac{x_{it}-\overline{x_{i}}}{s_i}; (t=1,2,..,T; i=1,2,..,n)$$ 

  - Aplicamos o algoritmo **DTW Barycenter** nas séries, utilizando os K-Vizinhos mais próximos, utilizando K=(2,3,4,...,10). E então, utilizamos nos resultados alguns **Índices de Validação de Cluster (CVI)** como o **Coeficiente da Silhueta** e o **Índice de Dunn**, que retornaram K=4 como sendo o número ideal de Clusters;
  
  - Obtendo os clusters de cada município, classificamos essa coluna como sendo um fator;
  
  - Aplicamos 4 métodos de classificação utilizando como variáveis explicativas alguns índices de cada município (população(2010), IDHM(2010), GINI(2010), Resultado Eleições (2018)), pórem nenhum obteve uma acurácia maior do que 55%.



# O que é Dynamic Time Warping?

A técnica permite que você descubra como alinhar dois sinais que podem ser de durações diferentes. Ela basicamente lhe devolve duas coisas:

  - Quais pontos em um dos sinais correspondem aos pontos do outro sinal;

  - O quanto dois sinais são similares entre si;
  
  O DTW é um algoritmo que basicamente nos diz como alinhar dois sinais de durações diferentes.
  
  
```{r}
knitr::include_graphics("Euclidean_vs_DTW.jpg")
```



<br>

# Importando o banco de dados do Wesley Cota

```{r}
# Importando os dados diarios
dados_temporais20 <- read.csv("cases-brazil-cities-time_2020.csv")
dados_temporais21 <- read.csv("cases-brazil-cities-time_2021.csv")
dados_temporais22 <- read.csv("cases-brazil-cities-time_2022.csv")

# Juntando os 3 anos em um unico dataset
dados_temporais <- rbind(dados_temporais20, dados_temporais21, dados_temporais22)

# Removendo do environment os anos separados para liberar memoria
rm(dados_temporais20)
rm(dados_temporais21)
rm(dados_temporais22)

# Removendo as linhas de TOTAIS  e os Casos sem localização definida
dados_temporais <- dados_temporais %>% 
                      filter(!grepl("TOTAL", city), !grepl("CASO SEM L", city))

# Vamos considerar apenas municipios com +100mil habitantes (grande porte)

## Para isso vamos ter que importar os dados pontuais do IBGE do censo de 2010
dados_pontuais <- readxl::read_xlsx("Data_municipality_variables.xlsx")
dados_pontuais <- dados_pontuais[-1,]

## Vetor contendo apenas o código dos municipios com +100 habitantes
muni_grande_porte <- dados_pontuais #%>% filter(Population > 100000) %>% select(Codmun6)
dados_temporais$ibgeID <- substr(dados_temporais$ibgeID, 1, 6) # removendo o ultimo digito do codigo do IBGE

# Separando outro dataset apenas com os municipios que em 2010 tinham pop. > 100k habitantes
dados_temporais2 <- dados_temporais %>% filter(ibgeID %in% muni_grande_porte$Codmun6)

#load("TCC_v01.RData")
```



# Por SEMANA EPIDEMIOLÓGICA

```{r fig.align='center'}
dados_semanais <- dados_temporais2 %>% 
                          group_by(epi_week, city) %>% 
                          summarise("newCases" = sum(newCases))


# Separando a série temporal de novos casos de SP
serie_semanal <- dados_semanais[dados_semanais$city == "Porto Alegre/RS", "newCases"]

# Normalizando os dados da serie temporal para deixar as cidades padronizadas
media <- mean(serie_semanal$newCases)
desvio_padrao <- sd(serie_semanal$newCases)
serie_semanal_padronizada <- (serie_semanal$newCases - media) / desvio_padrao



plot.ts(serie_semanal_padronizada)

#dados_temporais2[dados_temporais2$city == "São Félix do Xingu/PA",]
```



```{r fig.align='center'}
dados_semanais_pd <- dados_semanais
for (i in unique(dados_semanais$city)) {
  serie_temporal <- dados_semanais[dados_semanais$city == i, "newCases"]

  # Normalizando os dados da serie temporal para deixar as cidades padronizadas
  media <- mean(serie_temporal$newCases)
  desvio_padrao <- sd(serie_temporal$newCases)
  serie_temporal$newCases <- (serie_temporal$newCases - media) / desvio_padrao
  
  # Substituir os dados padronizados de volta no data frame original
  #dados_semanais_pd[dados_semanais_pd$city == i, "newCases"] <- serie_temporal
  dados_semanais_pd$newCases[dados_semanais_pd$city == i] <- serie_temporal$newCases

  #dados_semanais_pd <- dados_semanais %>% mutate(ifelse(city == i, serie_temporal$newCases, newCases))
}


# Dados reorganizados
dados_semanais_reorganizados <- dados_semanais_pd %>%
  pivot_wider(names_from = epi_week, values_from = newCases, values_fill = 0)


df_final3 <- as.matrix(dados_semanais_reorganizados[,c(2:150)])
rownames(df_final3) <- dados_semanais_reorganizados$city


head(df_final3)
#agrupamento3 <- uhclust(df_final3)
```

<br>

# DTW

```{r fig.align='center'}
## Rodando a clusterização com DTW Barycenter com K=(2,3,4,..,10)
# cluster_assignments <- tsclust(df_final3, k = 2L:10L, seed = 8L, distance = "dtw_basic", centroid = "dba", norm = "L2", window.size = 5L)


# Compute the dissimilarity matrix using DTW
dist_matrix <- proxy::dist(df_final3, method = "DTW")


 tsclust(series, type = "h", k = 4L, distance = "L2", trace = TRUE, control = hierarchical_control(method = diana))


cluster_assignments <- tsclust(df_final3, k = 2L:10L, seed = 8L, distance = "dtw_basic", centroid = "dba", norm = "L2", window.size = 5L)


## Aplicando CVI 
#validacao_n_clusters <- sapply(cluster_assignments, cvi, type = "internal")

#validacao_n_clusters
```

> Quais métodos de CVI escolher para justificar a quantidade de clusters?



<br>

```{r fig.align='center'}
result <- tsclust(df_final3, k = 4L, seed = 8L, distance = "dtw_basic", centroid = "dba", norm = "L2", window.size = 5L)
plot(result)  # Plot density of clusters
```


<br>

```{r}
# Dataframe com os municipios e os resultado dos clusters
clusters <- data.frame("city" = rownames(df_final3), "Cluster" = result@cluster)

clusters <- inner_join(clusters,
                      dados_temporais2 %>% 
                        select(city, ibgeID) %>% 
                        group_by(city, ibgeID) %>% 
                        summarise(),
                      by = "city")

clusters <- inner_join(clusters,
                       dados_pontuais,
                       by = c("ibgeID"="Codmun6"))

head(clusters)
```


<br>

# Métodos de Classificação

```{r, warning = FALSE, message = FALSE}
#clusters

var_num <- names(clusters)[9:13]
var_quali <- names(clusters)[c(2,8)]

clusters[var_quali] <- lapply(clusters[var_quali], function(x) {as.factor(x)})
clusters[var_num] <- lapply(clusters[var_num], function(x) {as.numeric(x)})

clusters2 <- clusters[,c(2, 8:13)]

clusters2 <- na.omit(clusters2)
```


<br>

## Random Forest

```{r, warning = FALSE, message = FALSE}
set.seed(324507)
library(caret)
## Random Forest
sample <- sample(seq_len(nrow(clusters2)), size = floor(0.8 * nrow(clusters2)))
dados_treino <- clusters2[sample, ]
dados_teste  <- clusters2[-sample, ]

summary(clusters2)

controle <- trainControl(method = "cv", number = 5)

modelo1 <- train(Cluster ~ ., data = dados_treino, method = "rf", trControl = controle, metric = "Accuracy", set.seed = 0304)

predicoes1 <- predict(modelo1, newdata = dados_teste)
matriz_confusao <- confusionMatrix(predicoes1, dados_teste$Cluster)
print(matriz_confusao)
print(paste("Acurácia = ", " ", round(matriz_confusao[["overall"]][["Accuracy"]],4)," | " , "Erro de precisão ="," ", round((1 - matriz_confusao[["overall"]][["Accuracy"]]),4), sep = ""))
```


<br>

## SVM

```{r}
## SVM
modelo2 <- train(Cluster ~ ., data = dados_treino, method = "svmLinear", trControl = controle, metric = "Accuracy", set.seed = 304)

predicoes2 <- predict(modelo2, newdata = dados_teste)
matriz_confusao <- confusionMatrix(predicoes2, dados_teste$Cluster)
print(matriz_confusao)
print(paste("Acurácia = ", " ", round(matriz_confusao[["overall"]][["Accuracy"]],4)," | " , "Erro de precisão ="," ", round((1 - matriz_confusao[["overall"]][["Accuracy"]]),4), sep = ""))
```

<br>

## Boosting

```{r}
## Boosting
modelo3 <- train(Cluster ~ ., data = dados_treino, method = "gbm", trControl = controle, metric = "Accuracy")

predicoes3 <- predict(modelo3, newdata = dados_teste)
matriz_confusao <- confusionMatrix(predicoes3, dados_teste$Cluster)
print(matriz_confusao)
print(paste("Acurácia = ", " ", round(matriz_confusao[["overall"]][["Accuracy"]],4)," | " , "Erro de precisão ="," ", round((1 - matriz_confusao[["overall"]][["Accuracy"]]),4), sep = ""))
```



